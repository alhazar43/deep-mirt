\documentclass[11pt]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts,bm}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{natbib}

\title{%
  Multidimensional DKVMN with Multi-Head Attention\\
  as a Dynamic MIRT Model with Flexible IRT Heads
}
\author{}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Dynamic knowledge tracing models such as DKT/DKVMN are typically optimized for predictive accuracy in binary-response settings, whereas psychometric analysis requires an explicit latent trait space, principled item parameterizations, and support for polytomous outcomes.
We propose a multidimensional DKVMN architecture that (i) stores a concept-indexed value memory whose rows are $D$-dimensional latent trait states, (ii) uses multi-head dot-product attention to implement item-to-concept addressing and concept-to-concept refinement, and (iii) generates responses through a unified family of multidimensional IRT heads (2PL/3PL, GPCM, GRM, NRM, and mixed-format extensions) sharing a common latent space.
From a statistical perspective, the network implements a deterministic amortized filter for a dynamic MIRT model; the write operation can be interpreted as a learned, likelihood-informed update driven by IRT residuals (generalizing the familiar ``$r-p$'' signal to partial credit and other polytomous scores).
We outline identifiability constraints and alignment procedures that enable interpretable latent dimensions, and a research plan covering parameter recovery under synthetic dynamic MIRT and empirical evaluation on mixed-format KT/psychometric datasets.
\end{abstract}



\section{Motivation and Setting}

Let $i$ index learners, $t=1,\dots,T_i$ time steps, and $j=1,\dots,Q$ items.
At time $t$, learner $i$ is administered item $q_{i,t} \in \{1,\dots,Q\}$ and produces a response $r_{i,t}$ belonging to a discrete set $\mathcal{Y}_{q_{i,t}}$ (binary, ordered categories, nominal options, or multi-response codes).

\paragraph{KT view.}
In deep knowledge tracing, DKVMN models maintain a key memory $M^k$ and a value memory $M^v_{i,t}$, and learn a mapping
\[
  (q_{i,1:t}, r_{i,1:t-1}) \mapsto p(r_{i,t} \mid q_{i,t}, \text{history}),
\]
typically with a scalar logit $z_{i,t}$ and a logistic output.
The hidden state is a high-dimensional representation with no explicit psychometric semantics.

\paragraph{MIRT view.}
In MIRT, a learner is represented by a latent trait vector $\bm{\theta}_i \in \mathbb{R}^D$ and each item $j$ by item parameters $\psi_j$.
The response distribution is given by an item-specific model
\[
  p(r_{i,j} \mid \bm{\theta}_i, \psi_j),
\]
often in log-linear form.
This yields an interpretable and rotation-invariant latent space, but does not prescribe dynamics.

\paragraph{Objective.}
We want a model that is simultaneously:

\begin{itemize}[leftmargin=*]
  \item \emph{dynamic}: $\bm{\theta}_{i,t}$ evolves with the response sequence;
  \item \emph{memory-based}: dynamics are implemented by a DKVMN-type architecture with attention over concept slots;
  \item \emph{MIRT-consistent}: at each $t$, responses are generated by IRT heads acting on $\bm{\theta}_{i,t}$ with psychometrically meaningful item parameters;
  \item \emph{mixed-format}: binary, graded and nominal items share the same latent trait space.
\end{itemize}

The core design choice is to treat the DKVMN value memory as a \emph{concept-level MIRT state}, and MHSA as the mechanism that couples items, concepts, and time.

\section{Static MIRT in Log-Linear Form}

We summarize only the structure required for the later construction. Let $\bm{\theta} \in \mathbb{R}^D$ denote a latent trait vector.
For item $j$ with response set $\mathcal{Y}_j$, we model
\begin{equation}
  p_j(y \mid \bm{\theta})
  = \mathbb{P}(X_j = y \mid \bm{\theta}),
  \quad y \in \mathcal{Y}_j.
\end{equation}
A wide family of MIRT models fits
\begin{equation}
  \log p_j(y \mid \bm{\theta})
  = \mu_j(y) + \bm{a}_j(y)^\top \bm{\theta} - A_j(\bm{\theta}),
  \label{eq:loglinear-mirt}
\end{equation}
with category intercepts $\mu_j(y)$, loadings $\bm{a}_j(y)\in\mathbb{R}^D$, and log-partition $A_j$ ensuring normalization.
Many familiar models are special cases:

\begin{itemize}[leftmargin=*]
  \item \textbf{2PL / 3PL}:
    $\mathcal{Y}_j=\{0,1\}$, $\bm{a}_j(1)=\bm{a}_j$, $\bm{a}_j(0)=\bm{0}$, $\mu_j(1)=-b_j$, $\mu_j(0)=0$, optionally with a lower asymptote.
  \item \textbf{GPCM / GRM}: step or cumulative thresholds; still log-linear after reparameterization.
  \item \textbf{Nominal}: distinct $\bm{a}_{jc}$ and $\mu_{jc}$ per category $c$.
\end{itemize}

\paragraph{Invariances.}
If $\bm{\theta}' = B\bm{\theta} + \bm{c}$ with invertible $B$ and vector $\bm{c}$, then
\begin{equation}
  \bm{a}_j'(y) = B^{-\top} \bm{a}_j(y), \qquad
  \mu_j'(y) = \mu_j(y) + \bm{a}_j(y)^\top B^{-1}\bm{c}
  \label{eq:mirt-affine}
\end{equation}
yields exactly the same probabilities.
Static models fix this indeterminacy by constraints such as
\begin{equation}
  \mathbb{E}[\bm{\theta}] = \bm{0}, \qquad
  \mathrm{Cov}(\bm{\theta}) = I_D,
  \label{eq:theta-standard}
\end{equation}
and, if desired, structural restrictions on $\bm{a}_j(y)$.

In the dynamic setting, \eqref{eq:mirt-affine} persists at each $t$, and we must keep it in mind when designing and interpreting the architecture.

\section{Dynamic MIRT Viewpoint}

A dynamic MIRT model assumes a latent trait trajectory
\[
  \bm{\theta}_{i,1:T_i} = (\bm{\theta}_{i,1},\dots,\bm{\theta}_{i,T_i}),
\]
with some prior $p(\bm{\theta}_{i,1:T_i})$ and observation model
\begin{equation}
  \mathbb{P}(r_{i,t} = y \mid \bm{\theta}_{i,t}, q_{i,t}=j)
  = p_j(y \mid \bm{\theta}_{i,t}), \quad y \in \mathcal{Y}_j.
  \label{eq:dyn-mirt-obs}
\end{equation}
Suppressing $i$ for clarity, the trajectory likelihood is
\begin{equation}
  p(\{(q_t,r_t)\}_{t=1}^T \mid \bm{\theta}_{1:T})
  = \prod_{t=1}^T p_{q_t}(r_t \mid \bm{\theta}_t).
\end{equation}

Rather than specifying $p(\bm{\theta}_t \mid \bm{\theta}_{t-1})$ parametrically, we use a memory network to learn a deterministic map
\begin{equation}
  \bm{\theta}_t = \Phi_\eta\big( \{(q_s,r_s)\}_{s \le t} \big),
  \label{eq:amortized-trace}
\end{equation}
where $\eta$ collects the memory parameters.
The DKVMN with MHSA is treated as a particular realization of $\Phi_\eta$.

\section{Multidimensional DKVMN with MHSA}

We now describe a multidimensional DKVMN whose value memory has $D$-dimensional rows and whose attention mechanisms are implemented with MHSA at two levels: item-to-concept and concept-to-concept.

Throughout this section we suppress the learner index $i$.

\subsection{Concept-level trait memory}

We assume $N$ latent concepts.
Their static embedding matrix is
\[
  M^k \in \mathbb{R}^{N \times d_k},
\]
which plays the role of a key memory and is shared across learners and time.
The dynamic value memory at time $t$ is
\begin{equation}
  M^v_t
  =
  \begin{bmatrix}
    \bm{s}_{1,t}^\top \\
    \vdots \\
    \bm{s}_{N,t}^\top
  \end{bmatrix}
  \in \mathbb{R}^{N \times D},
  \label{eq:value-memory}
\end{equation}
where $\bm{s}_{n,t} \in \mathbb{R}^D$ is the $D$-dimensional trait profile for concept $n$ at time $t$.

\paragraph{Relationship to classical DKVMN and to MIRT dimensionality.}
When $D=1$ and the attention mechanisms are single-head, \eqref{eq:value-memory} reduces to the standard DKVMN value memory with scalar concept mastery, and the update rule \eqref{eq:slot-update} becomes the familiar erase/add mechanism.
The multidimensionality here is explicit: $D$ is the MIRT trait dimension, while $N$ indexes a set of \emph{concept slots} used for content addressing.
Two interpretations are useful for psychometric work:

\begin{itemize}[leftmargin=*]
  \item \textbf{Concepts as dimensions (structured MIRT).}
    Set $D=N$ and constrain $\bm{s}_{n,t}=\theta_{n,t}\bm{e}_n$ (a scaled basis vector).
    Then the readout \eqref{eq:theta-from-memory} yields
    $
      \bm{\theta}_t = w_t \odot (\theta_{1,t},\dots,\theta_{N,t})
    $,
    i.e., an item-specific masking of a concept-proficiency vector.
    With suitable constraints on item loadings (Sec.~\ref{sec:concept-masked-discrimination}), this recovers a Q-matrix style MIRT where multi-KC items are handled naturally.
  \item \textbf{Concepts as a content-addressable basis (latent subspace).}
    For general $D\ll N$, the rows $\bm{s}_{n,t}\in\mathbb{R}^D$ form a learned basis of concept-conditioned trait vectors, and $w_t$ selects a convex combination relevant for the current item.
    This retains the flexibility of deep KT while keeping a fixed psychometric latent space.
\end{itemize}



We initialize $M^v_1$ either at zero or from a small Gaussian, and treat its evolution as deterministic given the interaction sequence.

\subsection{Item embeddings}

Each item $j$ has a one-hot code that is mapped to an embedding
\begin{equation}
  \bm{e}_j = E_q \bm{e}_j^{\text{onehot}} \in \mathbb{R}^{d_q},
\end{equation}
where $E_q \in \mathbb{R}^{d_q \times Q}$.
At time $t$, we write $\bm{e}_{q_t}$ simply as $\bm{e}_t$.

For some constructions below, we also use an item embedding $\bm{k}_j$ in the same space as $M^k$,
\begin{equation}
  \bm{k}_j = A^\top \bm{e}_j^{\text{onehot}} \in \mathbb{R}^{d_k},
\end{equation}
consistent with DKVMN.

\subsection{Multi-head item-to-concept read}

The key idea is to replace the scalar correlation weight in DKVMN by a multi-head dot-product attention over concepts.

For each head $h = 1,\dots,H$ we define
\begin{align}
  \bm{q}_t^{(h)} &= W_Q^{(h)} \bm{e}_t \in \mathbb{R}^{d_a}, \\
  \bm{k}_n^{(h)} &= W_K^{(h)} M^k(n) \in \mathbb{R}^{d_a},
\end{align}
with $M^k(n)$ the $n$th row of $M^k$.
The unnormalized scores are
\[
  u_t^{(h)}(n) = \frac{\big(\bm{q}_t^{(h)}\big)^\top \bm{k}_n^{(h)}}{\sqrt{d_a}},
\]
and the attention weights are
\begin{equation}
  \alpha_t^{(h)}(n)
  = \frac{\exp(u_t^{(h)}(n))}
         {\sum_{m=1}^N \exp(u_t^{(h)}(m))}.
\end{equation}
We define the aggregated concept weight as either an average or a learned convex combination across heads; for concreteness we use the simple average:
\begin{equation}
  w_t(n) = \frac{1}{H} \sum_{h=1}^H \alpha_t^{(h)}(n).
  \label{eq:agg-weight}
\end{equation}

\paragraph{Multi-KC structure and AKT-style recency bias (optional).}
The vector $w_t\in\Delta^{N-1}$ acts as a soft, item-dependent Q-vector.
If an expert Q-matrix $\tilde{Q}\in\{0,1\}^{Q\times N}$ is available, we can bias addressing by adding a logit prior
\[
  u_t^{(h)}(n) \leftarrow u_t^{(h)}(n) + \lambda_Q \log(\epsilon + \tilde{Q}_{q_t,n}),
\]
with a small $\epsilon>0$, so that the model can deviate from but not ignore the provided mapping.

Inspired by attentive KT, we may further incorporate a monotone decay term that discounts concepts not practiced recently.
Let $\Delta t_{n,t}$ denote the time since concept $n$ last received non-negligible attention weight.
Then we replace the raw score by
\[
  u_t^{(h)}(n) \leftarrow u_t^{(h)}(n) - \lambda^{(h)} \log\!\big(1+\Delta t_{n,t}\big),
\]
which implements a recency prior without changing the IRT head or the latent space.



The latent trait vector at time $t$ is then defined directly from the concept memory:
\begin{equation}
  \bm{\theta}_t = (M^v_t)^\top w_t
  = \sum_{n=1}^N w_t(n)\, \bm{s}_{n,t}
  \in \mathbb{R}^D.
  \label{eq:theta-from-memory}
\end{equation}
This replaces the scalar $\theta_t$ in 1D models by a vector in the same spirit, and makes the role of $M^v_t$ as a concept-level trait store explicit.

Optionally, one can post-process $\bm{\theta}_t$ by a shallow network,
\begin{equation}
  \bm{\theta}_t' = W_\theta \bm{\theta}_t + \bm{b}_\theta,
\end{equation}
but for clarity we treat $\bm{\theta}_t$ itself as the trait.

\subsection{Multi-head concept-to-concept refinement}

The read in \eqref{eq:theta-from-memory} already couples concepts via the attention weights $w_t$, but the concept states $\bm{s}_{n,t}$ themselves still evolve slot-wise.
To propagate information across concepts before generating $\bm{\theta}_t$, we introduce an MHSA layer acting over the $N$ concept rows.

Given $M^v_t \in \mathbb{R}^{N \times D}$, we form for head $h$:
\begin{align}
  Q_t^{(h)} &= M^v_t W_{Q_c}^{(h)} \in \mathbb{R}^{N \times d_c}, \\
  K_t^{(h)} &= M^v_t W_{K_c}^{(h)} \in \mathbb{R}^{N \times d_c}, \\
  V_t^{(h)} &= M^v_t W_{V_c}^{(h)} \in \mathbb{R}^{N \times d_c}.
\end{align}
Self-attention over concepts is
\begin{equation}
  A_t^{(h)} = \mathrm{softmax}\!\left( \frac{Q_t^{(h)} (K_t^{(h)})^\top}{\sqrt{d_c}} \right)
  \in \mathbb{R}^{N \times N},
\end{equation}
applied row-wise, and the refined states are
\[
  \tilde{M}^{v,(h)}_t = A_t^{(h)} V_t^{(h)} \in \mathbb{R}^{N \times d_c}.
\]
Concatenating heads and projecting back to $\mathbb{R}^{N \times D}$:
\begin{equation}
  \tilde{M}^v_t = \mathrm{concat}_h \tilde{M}^{v,(h)}_t \, W_{O_c},
\end{equation}
with $W_{O_c} \in \mathbb{R}^{(H d_c) \times D}$.
We then use a residual update with a small feed-forward $F_c$ and gating $g_c$:
\begin{align}
  F_t &= \mathrm{ReLU}\big( [M^v_t; \tilde{M}^v_t] W_F + b_F \big), \\
  g_t &= \sigma(M^v_t W_g + b_g), \\
  M^v_t &\leftarrow \mathrm{LayerNorm}\big( (1-g_t)\odot M^v_t + g_t \odot F_t \big).
  \label{eq:concept-mhsa}
\end{align}
\noindent where $W_F\in\mathbb{R}^{(2D)\times D}$ and $W_g\in\mathbb{R}^{D\times D}$ are applied row-wise (feature dimension), and $b_F,b_g\in\mathbb{R}^{D}$ are broadcast across the $N$ concept rows.
Equation \eqref{eq:concept-mhsa} parallels the stacked MHSA refinement previously applied to interaction embeddings, but here it operates directly on the concept-level trait matrix.

After this refinement at time $t$, the trait $\bm{\theta}_t$ is computed from the updated $M^v_t$ via \eqref{eq:theta-from-memory}.

\subsection{Multi-head write and memory evolution}

We now describe how $M^v_t$ evolves given the current interaction $(q_t,r_t)$.
We embed the interaction as
\[
  \bm{z}_t = \mathrm{embed}(q_t,r_t) \in \mathbb{R}^{d_w},
\]
for example via a learned embedding matrix over $(\text{item},\text{response})$ pairs.

For each head $h$, we define write queries and keys:
\begin{align}
  \bm{q}^{(h)}_{w,t} &= W_{Q_w}^{(h)} \bm{z}_t \in \mathbb{R}^{d_a}, \\
  \bm{k}_n^{(h)} &= W_{K_w}^{(h)} M^k(n) \in \mathbb{R}^{d_a},
\end{align}
and write weights
\begin{equation}
  \beta_t^{(h)}(n)
  =
  \frac{\exp\big( (\bm{q}^{(h)}_{w,t})^\top \bm{k}_n^{(h)} / \sqrt{d_a} \big)}
       {\sum_{m=1}^N \exp\big( (\bm{q}^{(h)}_{w,t})^\top \bm{k}_m^{(h)} / \sqrt{d_a} \big)}.
\end{equation}
Aggregating over heads as in \eqref{eq:agg-weight} gives a single write weight vector
\begin{equation}
  v_t(n) = \frac{1}{H} \sum_{h=1}^H \beta_t^{(h)}(n).
\end{equation}

We then update each concept state $\bm{s}_{n,t}$ via a gated additive rule:
\begin{align}
  \bm{u}_{n,t} &= U_s \bm{s}_{n,t} + U_z \bm{z}_t, \\
  \bm{e}_{n,t} &= \sigma(W_e \bm{u}_{n,t} + b_e), \\
  \bm{a}_{n,t} &= \tanh(W_a \bm{u}_{n,t} + b_a),
\end{align}
and
\begin{equation}
  \bm{s}_{n,t+1}
  = \bm{s}_{n,t} \odot \big(1 - v_t(n)\, \bm{e}_{n,t}\big)
    + v_t(n)\, \bm{a}_{n,t}.
  \label{eq:slot-update}
\end{equation}
Equations \eqref{eq:slot-update} generalize the scalar erase/add mechanism of DKVMN to $D$-dimensional states with head-averaged addressing.

Taken together, one update step is:\paragraph{Likelihood-consistent write for polytomous IRT heads (optional but recommended).}\label{sec:likelihood-consistent-write}
The generic interaction embedding $\bm{z}_t=\mathrm{embed}(q_t,r_t)$ is expressive but hides the connection between the write signal and the measurement model.
For psychometric interpretability, it is useful to expose the \emph{score residual} implied by the IRT head.
Let $\hat{p}_t(\cdot)=p_{q_t}(\cdot\mid\bm{\theta}_t)$ be the predictive distribution \emph{before} observing $r_t$, and let $s(\cdot)$ be a chosen scoring function (e.g., $s(k)=k$ for partial credit; any monotone category score is acceptable).
Define
\[
  \varepsilon_t = s(r_t)-\mathbb{E}_{\hat{p}_t}[s(R)].
\]
For binary items this reduces to $\varepsilon_t=r_t-\hat{p}_t(1)$, the usual ``$r-p$'' residual.
For the multidimensional GPCM \eqref{eq:gpcm-mirt} with $s(k)=k$, a direct calculation gives
\[
  \nabla_{\bm{\theta}_t}\log p_{q_t}(r_t\mid\bm{\theta}_t) = \bm{a}_{q_t}\,\varepsilon_t,
\]
so $\varepsilon_t$ is precisely the scalar driving the MIRT score function along the discrimination direction.

We incorporate $\varepsilon_t$ into the write in either of two ways:
(i) augment the interaction embedding, e.g.\ $\bm{z}_t=[\mathrm{embed}(q_t,r_t);\ \varepsilon_t]$; or
(ii) modulate the add term in \eqref{eq:slot-update} by $\varepsilon_t$, i.e.\ $\bm{a}_{n,t}\leftarrow \varepsilon_t\,\bm{a}_{n,t}$.
Optionally, one can scale $\varepsilon_t$ by a diagonal approximation of item information (Fisher) to obtain approximately equalized step sizes across heterogeneous item formats.
With small effective step sizes, \eqref{eq:slot-update} then behaves like a learned, concept-weighted (natural) gradient step on the dynamic MIRT log-likelihood, while retaining the representational benefits of DKVMN.


\begin{equation}
  M^v_{t+1} = \mathcal{U}_\phi\big(M^v_t, q_t, r_t\big),
\end{equation}
where $\phi$ collects all write and refinement parameters.
The latent trait is then read from $M^v_{t+1}$ at the next step via \eqref{eq:concept-mhsa} and \eqref{eq:theta-from-memory}.

\subsection{Intuition}

The combination of
\[
  M^v_1,\ \mathcal{U}_\phi,\ \text{MHSA over concepts},\ \text{item-to-concept attention}
\]
induces the mapping $\Phi_\eta$ in \eqref{eq:amortized-trace}.
Concept rows $\bm{s}_{n,t}$ approximate concept-specific trait components, the weights $w_t$ aggregate those components into an item-conditional trait vector, and the IRT head translates $\bm{\theta}_t$ into response probabilities.

The architecture is therefore naturally viewed as an amortized dynamic MIRT model with a DKVMN-based state evolution.

\section{IRT Heads and Item Parameter Output}

Given $\bm{\theta}_t$ and $q_t=j$, responses are generated via an item-specific head
\[
  p(r_t \mid \bm{\theta}_t, q_t=j)
  = p_{\tau_j}(r_t \mid \bm{\theta}_t; \psi_j),
\]
where $\tau_j$ encodes the item type and $\psi_j$ the item parameters.
All heads share the latent space $\mathbb{R}^D$.

\subsection{General log-linear head}

We first define a unified log-linear head for item $j$:
\begin{equation}
  \eta_{j,y}(\bm{\theta}_t)
  = \mu_j(y) + \bm{a}_j(y)^\top \bm{\theta}_t,
  \quad y \in \mathcal{Y}_j,
  \label{eq:eta-general}
\end{equation}
and
\begin{equation}
  p_{j}(y \mid \bm{\theta}_t)
  = \frac{\exp\big(\eta_{j,y}(\bm{\theta}_t)\big)}
         {\sum_{c \in \mathcal{Y}_j} \exp\big(\eta_{j,c}(\bm{\theta}_t)\big)}.
  \label{eq:softmax-general}
\end{equation}
Different IRT families are recovered by structural constraints on $\mu_j(\cdot)$ and $\bm{a}_j(\cdot)$.

\subsection{Item networks for parameter output}

We let the DKVMN share its item embedding with the IRT head.
For each item $j$:
\begin{align}
  \bm{k}_j &= A^\top \bm{e}_j^{\text{onehot}} \in \mathbb{R}^{d_k}, \\
  \bm{h}_j &= \mathrm{ReLU}(W_h \bm{k}_j + \bm{b}_h) \in \mathbb{R}^{d_h}.
\end{align}
All item parameters are outputs of shallow networks applied to $\bm{h}_j$; we denote the discrimination projection by $W_{a}^{\mathrm{irt}}$ to distinguish it from the write-network parameter $W_a$ in Eq.~\eqref{eq:slot-update}.

\paragraph{Concept-masked discriminations and optional Q-matrix anchoring.}\label{sec:concept-masked-discrimination}
To connect the KT notion of multi-KC structure to classical MIRT, we optionally couple the item discrimination to the item-to-concept addressing implied by the key memory.
Define the \emph{static} concept weights for item $j$ by applying the read attention to the item embedding:
\[
  w_j(n)=\frac{1}{H}\sum_{h=1}^H
  \mathrm{softmax}_n\!\left(\frac{(W_Q^{(h)}\bm{e}_j)^\top (W_K^{(h)}M^k(n))}{\sqrt{d_a}}\right).
\]
When $D=N$ (concepts-as-dimensions), a parsimonious structured MIRT parameterization is
\[
  \bm{a}_j = \mathrm{softplus}(W_{a}^{\mathrm{irt}} \bm{h}_j)\odot w_j,
\]
so discriminations are non-negative and sparse in the concept basis.
For general $D$, we map the concept simplex to the trait space by $\rho(w_j)=\mathrm{softplus}(W_c w_j)$ (or a low-rank factorization), and set
\[
  \bm{a}_j = \mathrm{softplus}(W_{a}^{\mathrm{irt}} \bm{h}_j)\odot \rho(w_j).
\]
If an expert Q-matrix is available, we can regularize $w_j$ toward it (e.g., entropy and KL penalties), making the learned dimensions more interpretable without hard-coding the mapping. To improve approximate identifiability in the fully neural setting, we recommend:
\begin{itemize}[leftmargin=*]
  \item \textbf{Anchors and sign conventions.}
    Enforce $\bm{a}_j \ge 0$ via softplus and optionally anchor a small set of items per dimension (or fix one loading per dimension) to reduce sign/rotation ambiguities. In the concepts-as-dimensions case, encourage sparsity in $\bm{a}_j$ (or in $w_j$) via an $\ell_1$ penalty or an entropy penalty.
  \item \textbf{Attentional identifiability.}
    Penalize overly diffuse addressing by adding an entropy regularizer on $w_t$ or $w_j$, and optionally penalize deviation from a provided Q-matrix when available.
\end{itemize}



\paragraph{Binary 2PL / 3PL.}
For $\mathcal{Y}_j=\{0,1\}$ we set
\begin{align}
  \bm{a}_j &= W_{a}^{\mathrm{irt}} \bm{h}_j, \\
  b_j &= \bm{w}_b^\top \bm{h}_j + b_b, \\
  \eta_{j,1}(\bm{\theta}_t) &= \bm{a}_j^\top \bm{\theta}_t - b_j, \\
  \eta_{j,0}(\bm{\theta}_t) &= 0,
\end{align}
so that
\begin{equation}
  p_j(1 \mid \bm{\theta}_t) = \sigma(\bm{a}_j^\top \bm{\theta}_t - b_j).
\end{equation}
A 3PL variant is obtained by adding a lower asymptote $c_j=\sigma(\bm{w}_c^\top \bm{h}_j)$ and defining
\[
  p_j(1 \mid \bm{\theta}_t)
  = c_j + (1-c_j)\, \sigma(\bm{a}_j^\top \bm{\theta}_t - b_j).
\]

\paragraph{Multidimensional GPCM.}
For ordered categories $k=0,\dots,K_j-1$, we use a common loading $\bm{a}_j$ and cumulative step difficulties.
Define
\begin{align}
  \bm{a}_j &= W_{a}^{\mathrm{irt}} \bm{h}_j, \\
  \delta_{j0} &= \bm{w}_0^\top \bm{h}_j + b_0, \\
  \Delta_{jh} &= \mathrm{softplus}(\bm{w}_h^\top \bm{h}_j + b_h),
  \quad h = 1,\dots,K_j-1, \\
  \delta_{jh} &= \delta_{j0} + \sum_{\ell=1}^h \Delta_{j\ell},
\end{align}
and set
\begin{equation}
  p_j(r_t = k \mid \bm{\theta}_t)
  =
  \frac{\exp\Big( \sum_{h=0}^{k} \big(\bm{a}_j^\top \bm{\theta}_t - \delta_{jh}\big) \Big)}
       {\sum_{c=0}^{K_j-1}
        \exp\Big( \sum_{h=0}^{c} \big(\bm{a}_j^\top \bm{\theta}_t - \delta_{jh}\big) \Big)}.
  \label{eq:gpcm-mirt}
\end{equation}
Equation \eqref{eq:gpcm-mirt} is the vector extension of the scalar GPCM head: in the special case $D=1$ and scalar $\bm{a}_j$, it collapses to the previous 1D formulation.

\paragraph{Graded Response Model (GRM).}
For GRM, we parameterize ordered thresholds $\gamma_{jk}$ via
\begin{align}
  \gamma_{j0} &= \bm{u}_0^\top \bm{h}_j + c_0, \\
  \Delta^{(\gamma)}_{jk} &= \mathrm{softplus}(\bm{u}_k^\top \bm{h}_j + c_k),
  \quad k=1,\dots,K_j-1, \\
  \gamma_{jk} &= \gamma_{j0} + \sum_{\ell=1}^k \Delta^{(\gamma)}_{j\ell}.
\end{align}
We keep a common loading $\bm{a}_j = W_{a}^{\mathrm{irt}} \bm{h}_j$ as above.
Define cumulative probabilities
\begin{equation}
  P(r_t \ge k \mid \bm{\theta}_t)
  = \sigma(\bm{a}_j^\top \bm{\theta}_t - \gamma_{jk}),
\end{equation}
and category probabilities
\begin{equation}
  p_j(r_t = k \mid \bm{\theta}_t)
  = P(r_t \ge k \mid \bm{\theta}_t) - P(r_t \ge k+1 \mid \bm{\theta}_t),
\end{equation}
with $P(r_t \ge 0)=1$ and $P(r_t \ge K_j)=0$.

\paragraph{Nominal Response Model (NRM).}
For unordered categories $c=1,\dots,C_j$ we assign category-specific loadings and intercepts:
\begin{align}
  \bm{\alpha}_{jc} &= W_{\alpha,c} \bm{h}_j, \\
  \beta_{jc} &= \bm{w}_{\beta,c}^\top \bm{h}_j + b_{\beta,c},
\end{align}
and define
\begin{equation}
  p_j(r_t = c \mid \bm{\theta}_t)
  =
  \frac{\exp(\bm{\alpha}_{jc}^\top \bm{\theta}_t + \beta_{jc})}
       {\sum_{d=1}^{C_j}
        \exp(\bm{\alpha}_{jd}^\top \bm{\theta}_t + \beta_{jd})}.
  \label{eq:nrm}
\end{equation}
The NRM is thus obtained as a particular instantiation of \eqref{eq:softmax-general}.

\paragraph{Multi-response items.}
For items allowing multiple selections or sub-scores, two strategies are possible:

\begin{itemize}[leftmargin=*]
  \item treat each sub-part as a separate item sharing $\bm{\theta}_t$ and its own $(\bm{a}_{j'},\mu_{j'})$;
  \item model the joint outcome via a higher-dimensional categorical with its own $(\mu_j(y),\bm{a}_j(y))$.
\end{itemize}

In both cases the parameter networks are shared, and the latent space is unchanged.

\section{Training Objective and Constraints}

\subsection{Sequence likelihood}

Given a sequence $\{(q_t,r_t)\}_{t=1}^T$, the model computes
\begin{equation}
  M^v_1
  \xrightarrow{\text{updates with } (q_1,r_1),\dots,(q_{t-1},r_{t-1})}
  M^v_t
  \xrightarrow{\text{MHSA + read}}
  \bm{\theta}_t
  \xrightarrow{\text{IRT head for } q_t}
  p(r_t \mid \bm{\theta}_t,q_t).
\end{equation}
The approximate log-likelihood for one learner is
\begin{equation}
  \mathcal{L}
  = \sum_{t=1}^T \log p_{q_t}(r_t \mid \bm{\theta}_t),
\end{equation}
and for a dataset we sum over learners and sequences.
The model is trained by maximising $\mathcal{L}$ (or minimizing the negative log-likelihood) via stochastic gradient descent.

\subsection{Optimization and engineering details}

In practice, KT data exhibit long-tailed item frequencies, missingness, and long sequences.
We therefore treat the architecture as a sequence model and adopt standard Transformer engineering choices:

\begin{itemize}[leftmargin=*]
  \item \textbf{Causal computation.}
    $\bm{\theta}_t$ is computed from $(q_{1:t},r_{1:t-1})$ only; the likelihood term at time $t$ uses the pre-update state to avoid label leakage.
  \item \textbf{Optimizer and schedule.}
    AdamW (decoupled weight decay), linear warmup followed by cosine decay, and global gradient clipping.
    Mixed precision is used when available.
  \item \textbf{Regularization.}
    Dropout on embeddings and attention weights, and (when stacking multiple refinement layers) stochastic depth.
    For high-cardinality polytomous heads we optionally use mild label smoothing; for severe class imbalance we consider focal reweighting.
  \item \textbf{Batching.}
    Variable-length sequences are padded with masks.
    For extremely long histories we optionally use truncated backpropagation through time.
\end{itemize}

These choices do not change the statistical model but materially improve stability and reproducibility.



\subsection{Regularization and approximate identifiability}

The static MIRT invariances in \eqref{eq:mirt-affine} extend to the dynamic case: for any invertible $B$ and $\bm{c}$, the transformed traits $\bm{\theta}_t'=B\bm{\theta}_t+\bm{c}$ and transformed item parameters produce identical likelihood.
With a flexible DKVMN mapping, $\Phi_\eta$ and the item networks can jointly rotate without changing predictions.

To obtain a stable and interpretable latent space we propose:

\begin{itemize}[leftmargin=*]
  \item \textbf{Standardization penalty.}
    Add penalties encouraging
    \[
      \frac{1}{N_{\text{tot}}} \sum_{i,t} \bm{\theta}_{i,t} \approx \bm{0}, 
      \qquad
      \frac{1}{N_{\text{tot}}} \sum_{i,t} \bm{\theta}_{i,t}\bm{\theta}_{i,t}^\top \approx I_D.
    \]
  \item \textbf{Post-hoc alignment.}
    After training, apply a Procrustes-type transformation to align $\bm{\theta}_{i,t}$ and loadings to a standard MIRT orientation for interpretability.
  \item \textbf{Structural constraints.}
    Use shared $\bm{a}_j$ across categories in graded models, ordered thresholds via softplus, and limited depth in item networks to keep parameterizations close to classical forms.
\end{itemize}

These steps do not alter predictive performance but make the dynamic traits comparable across runs and close to the usual MIRT conventions.

\section{Research Questions and Plan}

The proposed architecture raises technical questions at the intersection of deep sequence modeling and measurement theory.
We propose to study these questions using controlled synthetic data (where ground truth is known) and mixed-format real datasets.

\subsection{Theoretical and synthetic analysis}

We will simulate from a generative \emph{dynamic} MIRT model with known item parameters and latent evolution, including binary, graded, and nominal outcomes.
This enables parameter recovery and identifiability analyses that are not possible with observational KT datasets alone.

\begin{itemize}[leftmargin=*]
  \item \textbf{RQ1: Approximation of dynamic MIRT filtering.}
    To what extent can MHSA-DKVMN approximate the Bayes filter (or MAP filter) of a known dynamic MIRT model?
    How does recovery depend on $(N,D,H)$, refinement depth, and the presence/absence of time-decay bias?
  \item \textbf{RQ2: Rotation, scaling, and stability.}
    How do standardization penalties, discrimination constraints, and anchoring strategies affect latent orientation and training stability?
    What residual indeterminacy remains after penalties and post-hoc alignment?
  \item \textbf{RQ3: Mixed-format measurement.}
    When binary, partial credit, and nominal items co-exist, does using a shared latent space with type-specific heads improve calibration and parameter recovery compared with training separate models per format?
  \item \textbf{RQ4: Multi-KC structure vs.\ unconstrained MIRT.}
    How do concept-masked discriminations (Sec.~\ref{sec:concept-masked-discrimination}) trade off predictive performance and interpretability?
    When a Q-matrix is misspecified, how robust is the soft addressing mechanism?
  \item \textbf{RQ5: Likelihood-consistent write updates.}
    Does injecting IRT residuals (Sec.~\ref{sec:likelihood-consistent-write}) improve sample efficiency, calibration, and recovery of latent trajectories relative to a purely learned interaction embedding?
\end{itemize}

We will report (i) recovery of item parameters up to allowable affine transformations, (ii) RMSE/correlation of aligned latent trajectories, and (iii) predictive metrics (AUC/accuracy where applicable, log loss, and calibration).

\subsection{Real-data experiments}

On real KT and psychometric datasets with mixed formats (binary items, partial credit rubrics, Likert-style items, nominal options), we will:

\begin{itemize}[leftmargin=*]
  \item compare MHSA-DKVMN-MIRT against representative baselines:
    \begin{itemize}
      \item DKVMN with binary head and its mixed-format variants,
      \item DKT-style RNN baselines,
      \item attention-based KT models (e.g.\ AKT/Transformer-based KT),
      \item unidimensional neural IRT / Deep-IRT,
      \item static MIRT baselines where applicable;
    \end{itemize}
  \item evaluate predictive performance, calibration, and robustness to varying sequence lengths and item sparsity;
  \item analyze learned dimensions and item parameters, including alignment with external scales or known subtests, and specialization of attention heads to item types or concept clusters.
\end{itemize}

\section{Contributions}

This work aims to make deep knowledge tracing architectures usable as \emph{mathematical psychometric models} rather than purely predictive black boxes:

\begin{enumerate}[label=(\alph*),leftmargin=*]
  \item A mathematically explicit multidimensional DKVMN that treats the value memory as a concept-indexed trait state and whose read operation yields a dynamic MIRT latent vector.
  \item A unified mixed-format measurement layer: a log-linear MIRT family with concrete parameterizations for 2PL/3PL, GPCM, GRM, NRM, and multi-response strategies, with item parameters produced by a shared item network.
  \item Structural mechanisms that connect KT ``knowledge concepts'' to MIRT dimensions (concept-masked discriminations and optional Q-matrix anchoring), plus AKT-inspired recency bias for long sequences.
  \item A principled account of invariances and approximate identifiability in amortized dynamic MIRT, including standardization penalties, anchoring conventions, and post-hoc alignment procedures.
  \item Empirical evidence (synthetic recovery and real-data performance) characterizing the trade-offs between predictive accuracy, calibration, and interpretability when coupling MHSA-based memory networks with IRT measurement models.
\end{enumerate}

\bibliographystyle{plainnat}
% \bibliography{references}

\end{document}