\documentclass[11pt]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts,bm}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{natbib}

\title{%
  Multidimensional DKVMN\\
  as a Dynamic MIRT Model with a GPCM Head
}
\author{}
\date{\today}

\begin{document}

\maketitle

\section{Motivation and Setting}

Let $i$ index learners, $t=1,\dots,T_i$ time steps, and $j=1,\dots,Q$ items.
At time $t$, learner $i$ is administered item $q_{i,t} \in \{1,\dots,Q\}$ and produces a response $r_{i,t}$ belonging to a discrete set $\mathcal{Y}_{q_{i,t}}$ (binary, ordered categories, nominal options, or multi-response codes).

\paragraph{KT view.}
In deep knowledge tracing, DKVMN models maintain a key memory $M^k$ and a value memory $M^v_{i,t}$, and learn a mapping
\[
  (q_{i,1:t}, r_{i,1:t-1}) \mapsto p(r_{i,t} \mid q_{i,t}, \text{history}),
\]
typically with a scalar logit $z_{i,t}$ and a logistic output.
The hidden state is a high-dimensional representation with no explicit psychometric semantics.

\paragraph{MIRT view.}
In MIRT, a learner is represented by a latent trait vector $\bm{\theta}_i \in \mathbb{R}^D$ and each item $j$ by item parameters $\psi_j$.
The response distribution is given by an item-specific model
\[
  p(r_{i,j} \mid \bm{\theta}_i, \psi_j),
\]
often in log-linear form.
This yields an interpretable and rotation-invariant latent space, but does not prescribe dynamics.

\paragraph{Objective.}
We want a model that is simultaneously:

\begin{itemize}[leftmargin=*]
  \item \emph{dynamic}: $\bm{\theta}_{i,t}$ evolves with the response sequence;
  \item \emph{memory-based}: dynamics are implemented by a DKVMN-type architecture with attention over learned memory slots;
  \item \emph{MIRT-consistent}: at each $t$, responses are generated by a polytomous IRT head (GPCM) acting on $\bm{\theta}_{i,t}$ with item parameters;
  \item \emph{extensible}: the structure supports future ordinal heads (e.g., GRM) but the current implementation is GPCM-only.
\end{itemize}

The core design choice is to treat the DKVMN value memory as a \emph{slot-level MIRT state}, and attention as the mechanism that couples items, slots, and time.

\section{Static MIRT in Log-Linear Form}

We summarize only the structure required for the later construction. Let $\bm{\theta} \in \mathbb{R}^D$ denote a latent trait vector.
For item $j$ with response set $\mathcal{Y}_j$, we model
\begin{equation}
  p_j(y \mid \bm{\theta})
  = \mathbb{P}(X_j = y \mid \bm{\theta}),
  \quad y \in \mathcal{Y}_j.
\end{equation}
A wide family of MIRT models fits
\begin{equation}
  \log p_j(y \mid \bm{\theta})
  = \mu_j(y) + \bm{a}_j(y)^\top \bm{\theta} - A_j(\bm{\theta}),
  \label{eq:loglinear-mirt}
\end{equation}
with category intercepts $\mu_j(y)$, loadings $\bm{a}_j(y)\in\mathbb{R}^D$, and log-partition $A_j$ ensuring normalization.
Many familiar models are special cases:

\begin{itemize}[leftmargin=*]
  \item \textbf{2PL / 3PL}:
    $\mathcal{Y}_j=\{0,1\}$, $\bm{a}_j(1)=\bm{a}_j$, $\bm{a}_j(0)=\bm{0}$, $\mu_j(1)=-b_j$, $\mu_j(0)=0$, optionally with a lower asymptote.
  \item \textbf{GPCM / GRM}: step or cumulative thresholds; still log-linear after reparameterization.
  \item \textbf{Nominal}: distinct $\bm{a}_{jc}$ and $\mu_{jc}$ per category $c$.
\end{itemize}

\paragraph{Invariances.}
If $\bm{\theta}' = B\bm{\theta} + \bm{c}$ with invertible $B$ and vector $\bm{c}$, then
\begin{equation}
  \bm{a}_j'(y) = B^{-\top} \bm{a}_j(y), \qquad
  \mu_j'(y) = \mu_j(y) + \bm{a}_j(y)^\top B^{-1}\bm{c}
  \label{eq:mirt-affine}
\end{equation}
yields exactly the same probabilities.
Static models fix this indeterminacy by constraints such as
\begin{equation}
  \mathbb{E}[\bm{\theta}] = \bm{0}, \qquad
  \mathrm{Cov}(\bm{\theta}) = I_D,
  \label{eq:theta-standard}
\end{equation}
and, if desired, structural restrictions on $\bm{a}_j(y)$.

In the dynamic setting, \eqref{eq:mirt-affine} persists at each $t$, and we must keep it in mind when designing and interpreting the architecture.

\section{Dynamic MIRT Viewpoint}

A dynamic MIRT model assumes a latent trait trajectory
\[
  \bm{\theta}_{i,1:T_i} = (\bm{\theta}_{i,1},\dots,\bm{\theta}_{i,T_i}),
\]
with some prior $p(\bm{\theta}_{i,1:T_i})$ and observation model
\begin{equation}
  \mathbb{P}(r_{i,t} = y \mid \bm{\theta}_{i,t}, q_{i,t}=j)
  = p_j(y \mid \bm{\theta}_{i,t}), \quad y \in \mathcal{Y}_j.
  \label{eq:dyn-mirt-obs}
\end{equation}
Suppressing $i$ for clarity, the trajectory likelihood is
\begin{equation}
  p(\{(q_t,r_t)\}_{t=1}^T \mid \bm{\theta}_{1:T})
  = \prod_{t=1}^T p_{q_t}(r_t \mid \bm{\theta}_t).
\end{equation}

Rather than specifying $p(\bm{\theta}_t \mid \bm{\theta}_{t-1})$ parametrically, we use a memory network to learn a deterministic map
\begin{equation}
  \bm{\theta}_t = \Phi_\eta\big( \{(q_s,r_s)\}_{s \le t} \big),
  \label{eq:amortized-trace}
\end{equation}
where $\eta$ collects the memory parameters.
The current implementation uses a DKVMN with single-head slot attention as a realization of $\Phi_\eta$.

\section{Multidimensional DKVMN with Slot Attention}

We now describe a multidimensional DKVMN whose value memory has $D$-dimensional rows and whose attention is implemented with a single-head item-to-slot read.

Throughout this section we suppress the learner index $i$.

\subsection{Slot-level trait memory}

We assume $N$ learned memory slots (no explicit KC labels).
Their static embedding matrix is
\[
  M^k \in \mathbb{R}^{N \times d_k},
\]
which plays the role of a key memory and is shared across learners and time.
The dynamic value memory at time $t$ is
\begin{equation}
  M^v_t
  =
  \begin{bmatrix}
    \bm{s}_{1,t}^\top \\
    \vdots \\
    \bm{s}_{N,t}^\top
  \end{bmatrix}
  \in \mathbb{R}^{N \times D},
  \label{eq:value-memory}
\end{equation}
where $\bm{s}_{n,t} \in \mathbb{R}^D$ is the $D$-dimensional trait profile for slot $n$ at time $t$.

\paragraph{Relationship to classical DKVMN and to MIRT dimensionality.}
When $D=1$ and attention is single-head, \eqref{eq:value-memory} reduces to the standard DKVMN value memory with scalar slot mastery, and the update rule \eqref{eq:slot-update} becomes the familiar erase/add mechanism.
Here $D$ is the MIRT trait dimension, while $N$ indexes a set of \emph{memory slots} used for content addressing.
Two interpretations are useful for psychometric work:

\begin{itemize}[leftmargin=*]
  \item \textbf{Slots aligned with dimensions.}
    If we enforce $D=N$ (or tie $D$ to $N$) and set the value dimension to $D$, the readout can be interpreted as a direct mapping from slot activations to latent traits.
  \item \textbf{Slots as a content-addressable basis.}
    For general $D$ and $N$, the rows $\bm{s}_{n,t}\in\mathbb{R}^D$ form a learned basis of slot-conditioned trait vectors, and $w_t$ selects a convex combination relevant for the current item.
\end{itemize}



We initialize $M^v_1$ either at zero or from a small Gaussian, and treat its evolution as deterministic given the interaction sequence.

\subsection{Item embeddings}

Each item $j$ has a one-hot code that is mapped to an embedding
\begin{equation}
  \bm{e}_j = E_q \bm{e}_j^{\text{onehot}} \in \mathbb{R}^{d_q},
\end{equation}
where $E_q \in \mathbb{R}^{d_q \times Q}$.
At time $t$, we write $\bm{e}_{q_t}$ simply as $\bm{e}_t$.

For some constructions below, we also use an item embedding $\bm{k}_j$ in the same space as $M^k$,
\begin{equation}
  \bm{k}_j = A^\top \bm{e}_j^{\text{onehot}} \in \mathbb{R}^{d_k},
\end{equation}
consistent with DKVMN.

\subsection{Item-to-slot read}

The current implementation uses a single-head dot-product attention over slots.
Let $\bm{q}_t$ be the item embedding and $M^k$ the key memory. We compute
\begin{equation}
  w_t = \mathrm{softmax}\!\left( \bm{q}_t^\top M^k \right), \qquad
  r_t = \sum_{n=1}^N w_t(n)\, \bm{s}_{n,t}.
\end{equation}

We then form a summary representation by concatenating the read vector with the item embedding,
\begin{equation}
  \bm{h}_t = \tanh\!\left( W_h [r_t; \bm{q}_t] + \bm{b}_h \right),
\end{equation}
and extract $(\bm{\theta}_t, \bm{\alpha}_{q_t}, \bm{\beta}_{q_t})$ from $\bm{h}_t$ with a learned parameter head.
Optionally, $\bm{\theta}_t$ can be projected directly from the read vector when $D$ and the value dimension are aligned.

\subsection{Write and memory evolution}

The write step uses the same attention weights $w_t$ and a DKVMN-style erase/add update.
Let $\bm{v}_t$ be the embedded interaction (item, response), projected to the value dimension.
Then
\begin{equation}
  M^v_{t+1}(n) = M^v_t(n) \odot \big(1 - w_t(n)\, \bm{e}_t\big)
  + w_t(n)\, \bm{a}_t,
  \label{eq:slot-update}
\end{equation}
where $\bm{e}_t=\sigma(W_e \bm{v}_t)$ and $\bm{a}_t=\tanh(W_a \bm{v}_t)$ are the erase/add signals.
This preserves the classic DKVMN dynamics while keeping a multidimensional trait representation.

\subsection{Intuition}

The mapping $\Phi_\eta$ in \eqref{eq:amortized-trace} is implemented by the slot attention, summary readout, and DKVMN write update.
The slots store trait-relevant state, the attention weights select item-relevant slots, and the GPCM head maps $\bm{\theta}_t$ to polytomous response probabilities.

\section{IRT Heads and Item Parameter Output}

\noindent\textbf{Implementation status.}
The current codebase implements the GPCM head only; other heads below are retained as future extensions.

Given $\bm{\theta}_t$ and $q_t=j$, responses are generated via an item-specific head
\[
  p(r_t \mid \bm{\theta}_t, q_t=j)
  = p_{\tau_j}(r_t \mid \bm{\theta}_t; \psi_j),
\]
where $\tau_j$ encodes the item type and $\psi_j$ the item parameters.
All heads share the latent space $\mathbb{R}^D$.

\subsection{General log-linear head}

We first define a unified log-linear head for item $j$:
\begin{equation}
  \eta_{j,y}(\bm{\theta}_t)
  = \mu_j(y) + \bm{a}_j(y)^\top \bm{\theta}_t,
  \quad y \in \mathcal{Y}_j,
  \label{eq:eta-general}
\end{equation}
and
\begin{equation}
  p_{j}(y \mid \bm{\theta}_t)
  = \frac{\exp\big(\eta_{j,y}(\bm{\theta}_t)\big)}
         {\sum_{c \in \mathcal{Y}_j} \exp\big(\eta_{j,c}(\bm{\theta}_t)\big)}.
  \label{eq:softmax-general}
\end{equation}
Different IRT families are recovered by structural constraints on $\mu_j(\cdot)$ and $\bm{a}_j(\cdot)$.

\subsection{Parameter output in the current code}

The current implementation conditions item parameters on the time-step summary and the item embedding.
Concretely:
\begin{itemize}[leftmargin=*]
  \item $\bm{\theta}_t$ is produced from the summary state $\bm{h}_t$ (or optionally from the read vector when value and trait dimensions are aligned).
  \item $\bm{\alpha}_{q_t}$ is produced from $[\bm{h}_t;\bm{q}_t]$ via a shallow network with a positive transform.
  \item $\bm{\beta}_{q_t}$ is produced from the item embedding $\bm{q}_t$ and constructed as an ordered threshold vector.
\end{itemize}
This matches the implemented extractor in the current codebase and keeps item parameters conditioned on the active item and the time-step state.

\paragraph{Binary 2PL / 3PL.}
For $\mathcal{Y}_j=\{0,1\}$ we set
\begin{align}
  \bm{a}_j &= W_{a}^{\mathrm{irt}} \bm{h}_j, \\
  b_j &= \bm{w}_b^\top \bm{h}_j + b_b, \\
  \eta_{j,1}(\bm{\theta}_t) &= \bm{a}_j^\top \bm{\theta}_t - b_j, \\
  \eta_{j,0}(\bm{\theta}_t) &= 0,
\end{align}
so that
\begin{equation}
  p_j(1 \mid \bm{\theta}_t) = \sigma(\bm{a}_j^\top \bm{\theta}_t - b_j).
\end{equation}
A 3PL variant is obtained by adding a lower asymptote $c_j=\sigma(\bm{w}_c^\top \bm{h}_j)$ and defining
\[
  p_j(1 \mid \bm{\theta}_t)
  = c_j + (1-c_j)\, \sigma(\bm{a}_j^\top \bm{\theta}_t - b_j).
\]

\paragraph{Multidimensional GPCM.}
For ordered categories $k=0,\dots,K_j-1$, we use a common loading $\bm{a}_j$ and $K_j-1$ cumulative step difficulties.
Define
\begin{align}
  \bm{a}_j &= W_{a}^{\mathrm{irt}} \bm{h}_j, \\
  s_j &= \frac{\|\bm{a}_j\|_2}{\sqrt{D}}, \\
  \delta_{j0} &= \bm{w}_0^\top \bm{h}_j + b_0, \\
  \Delta_{jh} &= \mathrm{softplus}(\bm{w}_h^\top \bm{h}_j + b_h),
  \quad h = 1,\dots,K_j-2, \\
  \delta_{jh} &= \delta_{j0} + \sum_{\ell=1}^h \Delta_{j\ell},
\end{align}
and set the baseline category $k=0$ to have zero logit. Then
\begin{equation}
  p_j(r_t = k \mid \bm{\theta}_t)
  =
  \frac{\exp\Big( \sum_{h=0}^{k-1} \big(\bm{a}_j^\top \bm{\theta}_t - s_j \, \delta_{jh}\big) \Big)}
       {\sum_{c=0}^{K_j-1}
        \exp\Big( \sum_{h=0}^{c-1} \big(\bm{a}_j^\top \bm{\theta}_t - s_j \, \delta_{jh}\big) \Big)}.
  \label{eq:gpcm-mirt}
\end{equation}
Equation \eqref{eq:gpcm-mirt} is the vector extension of the standard K-1 step GPCM head: in the special case $D=1$ and scalar $\bm{a}_j$, it collapses to the usual 1D formulation.

\paragraph{Graded Response Model (GRM).}
For GRM, we parameterize ordered thresholds $\gamma_{jk}$ via
\begin{align}
  \gamma_{j0} &= \bm{u}_0^\top \bm{h}_j + c_0, \\
  \Delta^{(\gamma)}_{jk} &= \mathrm{softplus}(\bm{u}_k^\top \bm{h}_j + c_k),
  \quad k=1,\dots,K_j-1, \\
  \gamma_{jk} &= \gamma_{j0} + \sum_{\ell=1}^k \Delta^{(\gamma)}_{j\ell}.
\end{align}
We keep a common loading $\bm{a}_j = W_{a}^{\mathrm{irt}} \bm{h}_j$ as above.
Define cumulative probabilities
\begin{equation}
  P(r_t \ge k \mid \bm{\theta}_t)
  = \sigma(\bm{a}_j^\top \bm{\theta}_t - \gamma_{jk}),
\end{equation}
and category probabilities
\begin{equation}
  p_j(r_t = k \mid \bm{\theta}_t)
  = P(r_t \ge k \mid \bm{\theta}_t) - P(r_t \ge k+1 \mid \bm{\theta}_t),
\end{equation}
with $P(r_t \ge 0)=1$ and $P(r_t \ge K_j)=0$.

\paragraph{Nominal Response Model (NRM).}
For unordered categories $c=1,\dots,C_j$ we assign category-specific loadings and intercepts:
\begin{align}
  \bm{\alpha}_{jc} &= W_{\alpha,c} \bm{h}_j, \\
  \beta_{jc} &= \bm{w}_{\beta,c}^\top \bm{h}_j + b_{\beta,c},
\end{align}
and define
\begin{equation}
  p_j(r_t = c \mid \bm{\theta}_t)
  =
  \frac{\exp(\bm{\alpha}_{jc}^\top \bm{\theta}_t + \beta_{jc})}
       {\sum_{d=1}^{C_j}
        \exp(\bm{\alpha}_{jd}^\top \bm{\theta}_t + \beta_{jd})}.
  \label{eq:nrm}
\end{equation}
The NRM is thus obtained as a particular instantiation of \eqref{eq:softmax-general}.

\paragraph{Multi-response items.}
For items allowing multiple selections or sub-scores, two strategies are possible:

\begin{itemize}[leftmargin=*]
  \item treat each sub-part as a separate item sharing $\bm{\theta}_t$ and its own $(\bm{a}_{j'},\mu_{j'})$;
  \item model the joint outcome via a higher-dimensional categorical with its own $(\mu_j(y),\bm{a}_j(y))$.
\end{itemize}

In both cases the parameter networks are shared, and the latent space is unchanged.

\section{Training Objective and Constraints}

\subsection{Sequence likelihood}

Given a sequence $\{(q_t,r_t)\}_{t=1}^T$, the model computes
\begin{equation}
  M^v_1
  \xrightarrow{\text{updates with } (q_1,r_1),\dots,(q_{t-1},r_{t-1})}
  M^v_t
  \xrightarrow{\text{slot attention + summary read}}
  \bm{\theta}_t
  \xrightarrow{\text{IRT head for } q_t}
  p(r_t \mid \bm{\theta}_t,q_t).
\end{equation}
The approximate log-likelihood for one learner is
\begin{equation}
  \mathcal{L}
  = \sum_{t=1}^T \log p_{q_t}(r_t \mid \bm{\theta}_t),
\end{equation}
and for a dataset we sum over learners and sequences.
The model is trained by maximising $\mathcal{L}$ (or minimizing the negative log-likelihood) via stochastic gradient descent.

\subsection{Optimization and engineering details}

In practice, KT data exhibit long-tailed item frequencies, missingness, and long sequences.
The current implementation uses straightforward sequence-model training choices:

\begin{itemize}[leftmargin=*]
  \item \textbf{Causal computation.}
    $\bm{\theta}_t$ is computed from $(q_{1:t},r_{1:t-1})$ only; the likelihood term at time $t$ uses the pre-update state to avoid label leakage.
  \item \textbf{Optimizer.}
    AdamW with fixed learning rate and weight decay; no warmup schedule is currently applied.
  \item \textbf{Regularization.}
    Optional penalties include attention entropy, $\theta$ norm, and priors on $\alpha$ and $\beta$ (configurable weights).
  \item \textbf{Batching.}
    Variable-length sequences are padded with masks.
    Sequences are processed in full length with masking.
\end{itemize}

These choices do not change the statistical model but materially improve stability and reproducibility.



\subsection{Regularization and approximate identifiability}

The static MIRT invariances in \eqref{eq:mirt-affine} extend to the dynamic case: for any invertible $B$ and $\bm{c}$, the transformed traits $\bm{\theta}_t'=B\bm{\theta}_t+\bm{c}$ and transformed item parameters produce identical likelihood.
With a flexible DKVMN mapping, $\Phi_\eta$ and the item networks can jointly rotate without changing predictions.

To obtain a stable and interpretable latent space we propose:

\begin{itemize}[leftmargin=*]
  \item \textbf{Standardization penalty.}
    Add penalties encouraging
    \[
      \frac{1}{N_{\text{tot}}} \sum_{i,t} \bm{\theta}_{i,t} \approx \bm{0}, 
      \qquad
      \frac{1}{N_{\text{tot}}} \sum_{i,t} \bm{\theta}_{i,t}\bm{\theta}_{i,t}^\top \approx I_D.
    \]
  \item \textbf{Post-hoc alignment.}
    After training, apply a Procrustes-type transformation to align $\bm{\theta}_{i,t}$ and loadings to a standard MIRT orientation for interpretability.
  \item \textbf{Structural constraints.}
    Use shared $\bm{a}_j$ across categories in graded models, ordered thresholds via softplus, and limited depth in item networks to keep parameterizations close to classical forms.
\end{itemize}

These steps do not alter predictive performance but make the dynamic traits comparable across runs and close to the usual MIRT conventions.

\section{Research Questions and Plan}

The proposed architecture raises technical questions at the intersection of deep sequence modeling and measurement theory.
We propose to study these questions using controlled synthetic data (where ground truth is known) and mixed-format real datasets.

\subsection{Theoretical and synthetic analysis}

We will simulate from a generative \emph{dynamic} MIRT model with known item parameters and latent evolution, including binary, graded, and nominal outcomes.
This enables parameter recovery and identifiability analyses that are not possible with observational KT datasets alone.

\begin{itemize}[leftmargin=*]
  \item \textbf{RQ1: Approximation of dynamic MIRT filtering.}
    To what extent can the DKVMN-MIRT model approximate the Bayes filter (or MAP filter) of a known dynamic MIRT model?
    How does recovery depend on $(N,D)$, memory size, and the choice of $\theta$ source (summary vs.\ memory projection)?
  \item \textbf{RQ2: Rotation, scaling, and stability.}
    How do standardization penalties, discrimination constraints, and anchoring strategies affect latent orientation and training stability?
    What residual indeterminacy remains after penalties and post-hoc alignment?
  \item \textbf{RQ3: Mixed-format measurement.}
    When binary, partial credit, and nominal items co-exist, does using a shared latent space with type-specific heads improve calibration and parameter recovery compared with training separate models per format?
  \item \textbf{RQ4: Attention interpretability.}
    Do attention weights align with slot contributions to the MIRT score (e.g., $\theta$--attention correlation diagnostics), and how stable is this alignment across $D$?
  \item \textbf{RQ5: Regularization and stability.}
    How do attention entropy and trait normalization penalties affect convergence, calibration, and parameter recovery?
\end{itemize}

We will report (i) recovery of item parameters up to allowable affine transformations, (ii) RMSE/correlation of aligned latent trajectories, and (iii) predictive metrics (AUC/accuracy where applicable, log loss, and calibration).

\subsection{Real-data experiments}

On real KT and psychometric datasets with mixed formats (binary items, partial credit rubrics, Likert-style items, nominal options), we will:

\begin{itemize}[leftmargin=*]
  \item compare DKVMN-MIRT against representative baselines:
    \begin{itemize}
      \item DKVMN with binary head and its mixed-format variants,
      \item DKT-style RNN baselines,
      \item attention-based KT models (e.g.\ AKT/Transformer-based KT),
      \item unidimensional neural IRT / Deep-IRT,
      \item static MIRT baselines where applicable;
    \end{itemize}
  \item evaluate predictive performance, calibration, and robustness to varying sequence lengths and item sparsity;
  \item analyze learned dimensions and item parameters, including alignment with external scales or known subtests, and stability of attention patterns across item groups.
\end{itemize}

\section{Contributions}

This work aims to make deep knowledge tracing architectures usable as \emph{mathematical psychometric models} rather than purely predictive black boxes:

\begin{enumerate}[label=(\alph*),leftmargin=*]
  \item A multidimensional DKVMN that treats the value memory as a slot-indexed trait state and whose read operation yields a dynamic MIRT latent vector.
  \item A polytomous measurement layer with a concrete GPCM parameterization; other heads remain future extensions.
  \item A principled account of invariances and approximate identifiability in amortized dynamic MIRT, including standardization penalties and post-hoc alignment procedures.
  \item Empirical evidence (synthetic recovery and real-data performance) characterizing trade-offs between predictive accuracy, calibration, and interpretability in a slot-attention MIRT model.
\end{enumerate}

\bibliographystyle{plainnat}
% \bibliography{references}

\end{document}
