# large_5000_v6.yaml
#
# Matches deep-gpcm architecture: discrimination_network input is
# [summary, question_embed] (concat) instead of question_embed alone.
#
# Changes vs v5:
#   - discrimination_network input: summary_dim + key_dim = 50+64 = 114
#     (matches deep-gpcm's discrim_input = student_context + item_embedding)
#   - theta_norm_weight: 0.1 (back to v3 level â€” v5 showed 0.5 hurts alpha)
#   - alpha_prior_weight: 0.0 (no attractor toward alpha=1)
#   - epochs: 50

base:
  experiment_name: "large_5000_v6"
  device: "cuda"
  seed: 42

model:
  n_questions: 200
  n_categories: 5
  n_traits: 1
  memory_size: 100
  key_dim: 64
  value_dim: 128
  summary_dim: 50
  ability_scale: 1.0
  dropout_rate: 0.0
  memory_add_activation: "tanh"
  init_value_memory: true

training:
  epochs: 50
  batch_size: 64
  lr: 0.001
  grad_clip: 1.0
  focal_weight: 0.5
  weighted_ordinal_weight: 0.5
  ordinal_penalty: 0.5
  lr_patience: 10
  lr_factor: 0.9
  attention_entropy_weight: 0.0
  theta_norm_weight: 0.1
  alpha_prior_weight: 0.0
  beta_prior_weight: 0.0

data:
  data_dir: "data"
  dataset_name: "large_5000"
  train_split: 0.8
  min_seq_len: 10
