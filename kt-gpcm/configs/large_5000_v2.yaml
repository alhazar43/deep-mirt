# large_5000_v2.yaml
#
# Changes vs large_5000 run:
#   - Data generated with corrected M-GPCM formula (alpha*theta - beta, not alpha*(theta-beta))
#   - memory_size doubled (50 → 100) for richer ability tracking
#   - LR schedule slowed: patience 3→10, factor 0.8→0.9
#   - Identifiability regularisers enabled:
#       theta_norm_weight: anchors theta ~ N(0,1), prevents theta absorbing all alpha variance
#       alpha_prior_weight: pulls log(alpha) ~ N(0, 0.3), discourages all-ones collapse

base:
  experiment_name: "large_5000_v2"
  device: "cuda"
  seed: 42

model:
  n_questions: 200
  n_categories: 5
  n_traits: 1
  memory_size: 100
  key_dim: 64
  value_dim: 128
  summary_dim: 50
  ability_scale: 1.0
  dropout_rate: 0.0
  memory_add_activation: "tanh"
  init_value_memory: true

training:
  epochs: 50
  batch_size: 64
  lr: 0.001
  grad_clip: 1.0
  focal_weight: 0.5
  weighted_ordinal_weight: 0.5
  ordinal_penalty: 0.5
  lr_patience: 10
  lr_factor: 0.9
  attention_entropy_weight: 0.0
  theta_norm_weight: 0.05
  alpha_prior_weight: 0.1
  beta_prior_weight: 0.0

data:
  data_dir: "data"
  dataset_name: "large_5000_v2"
  train_split: 0.8
  min_seq_len: 10
