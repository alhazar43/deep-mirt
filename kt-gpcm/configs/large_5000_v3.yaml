# large_5000_v3.yaml
#
# Tests unit-variance theta penalty (trainer fix) with OLD scalar-GPCM data.
# This isolates the effect of the theta constraint on alpha recovery.
#
# Changes vs v2:
#   - theta_norm_penalty now enforces unit variance (mean=0, var=1)
#     rather than E[theta²] which pushed theta→0 (wrong!)
#   - dataset: large_5000 (scalar GPCM formula — intentional for ablation)
#   - theta_norm_weight: 0.1 (stronger anchor)
#   - alpha_prior_weight: 0.02 (lighter — let data drive alpha spread)

base:
  experiment_name: "large_5000_v3"
  device: "cuda"
  seed: 42

model:
  n_questions: 200
  n_categories: 5
  n_traits: 1
  memory_size: 100
  key_dim: 64
  value_dim: 128
  summary_dim: 50
  ability_scale: 1.0
  dropout_rate: 0.0
  memory_add_activation: "tanh"
  init_value_memory: true

training:
  epochs: 50
  batch_size: 64
  lr: 0.001
  grad_clip: 1.0
  focal_weight: 0.5
  weighted_ordinal_weight: 0.5
  ordinal_penalty: 0.5
  lr_patience: 10
  lr_factor: 0.9
  attention_entropy_weight: 0.0
  theta_norm_weight: 0.1
  alpha_prior_weight: 0.02
  beta_prior_weight: 0.0

data:
  data_dir: "data"
  dataset_name: "large_5000"
  train_split: 0.8
  min_seq_len: 10
